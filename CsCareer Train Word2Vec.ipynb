{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import codecs\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import argparse\n",
    "from gensim.models import phrases, word2vec\n",
    "import re\n",
    "import string\n",
    "from gensim.models import KeyedVectors\n",
    "import csv\n",
    "import statsmodels.formula.api as smf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ctoups/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"cscareerquestions.corpus/\"\n",
    "utterances_path = os.path.join(data_path, \"utterances.jsonl\")\n",
    "csv_path = os.path.join(data_path, \"RedditDataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a few minutes to load into a dataframe so best to avoid calling this unless you've just started the kernel\n",
    "first_run = True \n",
    "if first_run:\n",
    "#     df = pd.read_csv(csv_path, converters={\"tokenize\": lambda x: x.strip(\"[]\").split(\", \")})\n",
    "    df = pd.read_json(utterances_path, lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>root</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>meta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nyv4b</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>nyv4b</td>\n",
       "      <td>None</td>\n",
       "      <td>2012-01-01 20:25:05</td>\n",
       "      <td>I've been considering going back to school (I ...</td>\n",
       "      <td>{'score': 12, 'top_level_comment': None, 'retr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nytxd</td>\n",
       "      <td>fraincs</td>\n",
       "      <td>nytxd</td>\n",
       "      <td>None</td>\n",
       "      <td>2012-01-01 19:52:35</td>\n",
       "      <td>I've studied Graphic Design at school and to m...</td>\n",
       "      <td>{'score': 4, 'top_level_comment': None, 'retri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ny58i</td>\n",
       "      <td>Slaughtermatic</td>\n",
       "      <td>ny58i</td>\n",
       "      <td>None</td>\n",
       "      <td>2012-01-01 01:33:24</td>\n",
       "      <td>I'm looking at all the internships available f...</td>\n",
       "      <td>{'score': 1, 'top_level_comment': None, 'retri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ny45r</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>ny45r</td>\n",
       "      <td>None</td>\n",
       "      <td>2012-01-01 01:00:02</td>\n",
       "      <td>I am working in industry and plan to pursue a ...</td>\n",
       "      <td>{'score': 4, 'top_level_comment': None, 'retri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>o070l</td>\n",
       "      <td>m555</td>\n",
       "      <td>o070l</td>\n",
       "      <td>None</td>\n",
       "      <td>2012-01-02 21:13:15</td>\n",
       "      <td>I've been reading about the many ills of most ...</td>\n",
       "      <td>{'score': 13, 'top_level_comment': None, 'retr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985074</th>\n",
       "      <td>e8tkc2b</td>\n",
       "      <td>riddleadmiral</td>\n",
       "      <td>9swgn6</td>\n",
       "      <td>e8s07pb</td>\n",
       "      <td>2018-10-31 23:57:09</td>\n",
       "      <td>ask: How responsive are people to emails/Slack...</td>\n",
       "      <td>{'score': 2, 'top_level_comment': 'e8s07pb', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985075</th>\n",
       "      <td>e8tkcje</td>\n",
       "      <td>diduxchange</td>\n",
       "      <td>9t06mp</td>\n",
       "      <td>9t06mp</td>\n",
       "      <td>2018-10-31 23:57:22</td>\n",
       "      <td>&amp;gt; work in big-N (in Seattle no less)\\n&amp;gt; ...</td>\n",
       "      <td>{'score': 2, 'top_level_comment': 'e8tkcje', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985076</th>\n",
       "      <td>e8tke5v</td>\n",
       "      <td>istareatscreens</td>\n",
       "      <td>9t1ufl</td>\n",
       "      <td>9t1ufl</td>\n",
       "      <td>2018-10-31 23:58:08</td>\n",
       "      <td>\"will I be severely stunted in programming and...</td>\n",
       "      <td>{'score': 1, 'top_level_comment': 'e8tke5v', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985077</th>\n",
       "      <td>e8tkeud</td>\n",
       "      <td>dopkick</td>\n",
       "      <td>9t2gph</td>\n",
       "      <td>9t2gph</td>\n",
       "      <td>2018-10-31 23:58:27</td>\n",
       "      <td>Jobs 2+ have all been via contacts for me.</td>\n",
       "      <td>{'score': 7, 'top_level_comment': 'e8tkeud', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985078</th>\n",
       "      <td>e8tkglm</td>\n",
       "      <td>jo3z3f</td>\n",
       "      <td>9t2gph</td>\n",
       "      <td>e8tj84b</td>\n",
       "      <td>2018-10-31 23:59:17</td>\n",
       "      <td>Truth!</td>\n",
       "      <td>{'score': 1, 'top_level_comment': 'e8tj84b', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1985079 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id             user    root reply_to           timestamp  \\\n",
       "0          nyv4b        [deleted]   nyv4b     None 2012-01-01 20:25:05   \n",
       "1          nytxd          fraincs   nytxd     None 2012-01-01 19:52:35   \n",
       "2          ny58i   Slaughtermatic   ny58i     None 2012-01-01 01:33:24   \n",
       "3          ny45r        [deleted]   ny45r     None 2012-01-01 01:00:02   \n",
       "4          o070l             m555   o070l     None 2012-01-02 21:13:15   \n",
       "...          ...              ...     ...      ...                 ...   \n",
       "1985074  e8tkc2b    riddleadmiral  9swgn6  e8s07pb 2018-10-31 23:57:09   \n",
       "1985075  e8tkcje      diduxchange  9t06mp   9t06mp 2018-10-31 23:57:22   \n",
       "1985076  e8tke5v  istareatscreens  9t1ufl   9t1ufl 2018-10-31 23:58:08   \n",
       "1985077  e8tkeud          dopkick  9t2gph   9t2gph 2018-10-31 23:58:27   \n",
       "1985078  e8tkglm           jo3z3f  9t2gph  e8tj84b 2018-10-31 23:59:17   \n",
       "\n",
       "                                                      text  \\\n",
       "0        I've been considering going back to school (I ...   \n",
       "1        I've studied Graphic Design at school and to m...   \n",
       "2        I'm looking at all the internships available f...   \n",
       "3        I am working in industry and plan to pursue a ...   \n",
       "4        I've been reading about the many ills of most ...   \n",
       "...                                                    ...   \n",
       "1985074  ask: How responsive are people to emails/Slack...   \n",
       "1985075  &gt; work in big-N (in Seattle no less)\\n&gt; ...   \n",
       "1985076  \"will I be severely stunted in programming and...   \n",
       "1985077         Jobs 2+ have all been via contacts for me.   \n",
       "1985078                                             Truth!   \n",
       "\n",
       "                                                      meta  \n",
       "0        {'score': 12, 'top_level_comment': None, 'retr...  \n",
       "1        {'score': 4, 'top_level_comment': None, 'retri...  \n",
       "2        {'score': 1, 'top_level_comment': None, 'retri...  \n",
       "3        {'score': 4, 'top_level_comment': None, 'retri...  \n",
       "4        {'score': 13, 'top_level_comment': None, 'retr...  \n",
       "...                                                    ...  \n",
       "1985074  {'score': 2, 'top_level_comment': 'e8s07pb', '...  \n",
       "1985075  {'score': 2, 'top_level_comment': 'e8tkcje', '...  \n",
       "1985076  {'score': 1, 'top_level_comment': 'e8tke5v', '...  \n",
       "1985077  {'score': 7, 'top_level_comment': 'e8tkeud', '...  \n",
       "1985078  {'score': 1, 'top_level_comment': 'e8tj84b', '...  \n",
       "\n",
       "[1985079 rows x 7 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          I've been considering going back to school (I ...\n",
       "1          I've studied Graphic Design at school and to m...\n",
       "2          I'm looking at all the internships available f...\n",
       "3          I am working in industry and plan to pursue a ...\n",
       "4          I've been reading about the many ills of most ...\n",
       "                                 ...                        \n",
       "1985150    ask: How responsive are people to emails/Slack...\n",
       "1985151    &gt; work in big-N (in Seattle no less)\\n&gt; ...\n",
       "1985152    \"will I be severely stunted in programming and...\n",
       "1985153           Jobs 2+ have all been via contacts for me.\n",
       "1985154                                               Truth!\n",
       "Name: text, Length: 1985155, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectors of words\n",
    "def read_from_txt(filename):\n",
    "    result = []\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            result.append(line.strip())\n",
    "    return result\n",
    "\n",
    "man_words =  ['he', 'him', 'hes', 'his', 'himself','man', 'boy', 'male', 'men', 'boys', 'males']\n",
    "woman_words = [ 'she', 'shes', 'her', 'hers', 'herself', 'woman', 'girl', 'female', 'women','girls', 'females']\n",
    "combined = man_words + woman_words\n",
    "adjectives_list = set(read_from_txt('adjectives_garg.txt') + read_from_txt('adjectives_princeton.txt'))\n",
    "personal_list = ['qualified', 'under qualified', 'unqualified']\n",
    "competence_adjectives = read_from_txt('adjectives_intelligencegeneral.txt')\n",
    "vocab = read_from_txt(os.path.join(data_path, 'RedditVocab.txt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mustn', 'down', 'yourself', 'has', 'its', 'y', 'haven', 'until', 'myself', 'only', 'nor', 'couldn', 'any', 'didn', 'our', 'where', 'other', 'ma', 'did', 'by', 'of', 'such', 've', 'off', 'on', \"shan't\", \"don't\", 'wasn', 'hadn', 'their', 'do', 'are', 'there', 'should', 'the', 're', \"mightn't\", 'as', 'and', 'ourselves', 'when', 'yours', 'up', 'your', 'they', 'have', 'after', 'own', 'can', 'me', 'again', 'same', 't', \"doesn't\", 'been', \"needn't\", 'over', 'does', 'above', 'needn', \"she's\", 'here', 'mightn', 'in', \"isn't\", 'ain', 'most', 'then', 'wouldn', \"should've\", 'those', 'what', 'than', 'not', 'am', 'into', 'each', 'doing', 'be', 'during', 'o', 'some', 'had', \"you'll\", 'my', 'through', 'while', 'll', \"that'll\", 'under', \"hasn't\", 'we', 'an', 'no', 'isn', \"you'd\", 'because', 'out', 'all', 'why', 'these', 'but', \"aren't\", \"wasn't\", 'that', 'is', 'doesn', 'was', 's', \"wouldn't\", 'm', 'once', 'about', \"haven't\", \"didn't\", 'ours', 'or', 'don', 'now', \"mustn't\", 'won', \"you're\", 'shouldn', 'for', 'having', 'which', 'between', \"weren't\", 'being', 'before', 'who', 'so', \"couldn't\", 'yourselves', 'them', 'too', 'to', 'whom', \"it's\", 'below', 'from', 'more', \"won't\", 'd', 'were', 'very', 'aren', 'itself', 'themselves', \"shouldn't\", 'further', 'i', 'you', 'if', 'will', 'just', \"hadn't\", 'both', 'this', 'a', 'shan', 'how', 'hasn', 'few', 'it', 'against', 'weren', 'theirs', \"you've\", 'with', 'at'}\n"
     ]
    }
   ],
   "source": [
    "stops = set(stopwords.words('english')).difference(set(combined))\n",
    "print(stops)\n",
    "punct_chars = list((set(string.punctuation) | {'»', '–', '—', '-',\"­\", '\\xad', '-', '◾', '®', '©','✓','▲', '◄','▼','►', '~', '|', '“', '”', '…', \"'\", \"`\", '_', '•', '*', '■'} - {\"'\"}))\n",
    "punct_chars.sort()\n",
    "punctuation = ''.join(punct_chars)\n",
    "replace = re.compile('[%s]' % re.escape(punctuation))\n",
    "sno = nltk.stem.SnowballStemmer('english')\n",
    "printable = set(string.printable)\n",
    "\n",
    "def clean_text(text,\n",
    "               remove_stopwords=True,\n",
    "               remove_numeric=True,\n",
    "               stem=False,\n",
    "               remove_short=False):\n",
    "    # lower case\n",
    "    text = text.lower()\n",
    "    # eliminate urls\n",
    "    text = re.sub(r'http\\S*|\\S*\\.com\\S*|\\S*www\\S*', ' ', text)\n",
    "    # substitute all other punctuation with whitespace\n",
    "    text = replace.sub(' ', text)\n",
    "    # replace all whitespace with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # strip off spaces on either end\n",
    "    text = text.strip()\n",
    "    # make sure all chars are printable\n",
    "    text = ''.join([c for c in text if c in printable])\n",
    "    words = text.split()\n",
    "    if remove_stopwords:\n",
    "        words = [w for w in words if w not in stops]\n",
    "    if remove_numeric:\n",
    "        words = [w for w in words if not w.isdigit()]\n",
    "    if stem:\n",
    "        words = [sno.stem(w) for w in words]\n",
    "    if remove_short:\n",
    "        words = [w for w in words if len(w) >= 3]\n",
    "    return words\n",
    "\n",
    "def tokenize(text):\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    return [clean_text(s, stem=False, remove_stopwords=True, remove_short=False) for s in sents]\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n"
     ]
    }
   ],
   "source": [
    "all_sentences = []\n",
    "for i, text in enumerate(df[\"text\"]):\n",
    "    if type(text) != str:\n",
    "        continue\n",
    "    all_sentences.extend(tokenize(text))\n",
    "    if i % 100000 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7555245"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = phrases.Phrases(all_sentences, min_count=5, delimiter=b' ', common_terms=stops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocabulary...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating vocabulary...\")\n",
    "vocab = [w for sent in bigrams[all_sentences] for w in sent]\n",
    "vocab_count = Counter(vocab)\n",
    "vocab = [w for w, count in counted.most_common() if count >= 5]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"vocab_count.pkl\",\"wb\")\n",
    "pickle.dump(vocab_count,f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Vocab\n",
    "with codecs.open(os.path.join(data_path, 'RedditVocab.txt'), 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(vocab))\n",
    "\n",
    "#Save Loaded Dataframe into CSV\n",
    "df.to_csv(os.path.join(data_path, 'RedditDataframe.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run #0\n"
     ]
    }
   ],
   "source": [
    "num_runs = 1 \n",
    "dims = 100\n",
    "bootstrap = False\n",
    "window = 5\n",
    "data = bigrams[all_sentences]\n",
    "model = word2vec.Word2Vec(data, size=dims, window=window, sg=1, min_count=5)\n",
    "model.wv.save(os.path.join(data_path, str(run_idx) + '.wv'))\n",
    "vectors = model.wv\n",
    "print('FINISHED')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
